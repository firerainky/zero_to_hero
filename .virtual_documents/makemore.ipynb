import torch
import matplotlib.pyplot as plt
%matplotlib inline





words = open('names.txt', 'r').read().splitlines()
words[:10]


len(words)


min(len(w) for w in words)


max(len(w) for w in words)





b = {}
for w in words:
    chs =['<S>'] + list(w) + ['<E>']
    for ch1, ch2 in zip(chs, chs[1:]):
        bigram = (ch1, ch2)
        b[bigram] = b.get(bigram, 0) + 1


# sorted(b.items(), key = lambda kv: -kv[1])





N = torch.zeros((28, 28), dtype = torch.int32)


chars = sorted(list(set(''.join(words))))
stoi = {s:i for i, s in enumerate(chars)}
stoi['<S>'] = 26
stoi['<E>'] = 27


for w in words:
    chs =['<S>'] + list(w) + ['<E>']
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        N[ix1, ix2] += 1





itos = {i:s for s, i in stoi.items()}


plt.figure(figsize=(16, 16))
plt.imshow(N, cmap='Blues')
for i in range(28):
    for j in range(28):
        chstr = itos[i] + itos[j]
        plt.text(j, i, chstr, ha = 'center', va = 'bottom', color = 'gray')
        plt.text(j, i, N[i, j].item(), ha = 'center', va = 'top', color = 'gray')
plt.axis('off')


# deleting spurious (S) and (E) tokens in favor of a single . token


N = torch.zeros((27, 27), dtype = torch.int32)


chars = sorted(list(set(''.join(words))))
stoi = {s:i+1 for i, s in enumerate(chars)}
stoi['.'] = 0
itos = {i:s for s, i in stoi.items()}
itos


for w in words:
    chs =['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        N[ix1, ix2] += 1


plt.figure(figsize=(16, 16))
plt.imshow(N, cmap='Blues')
for i in range(27):
    for j in range(27):
        chstr = itos[i] + itos[j]
        plt.text(j, i, chstr, ha = 'center', va = 'bottom', color = 'gray')
        plt.text(j, i, N[i, j].item(), ha = 'center', va = 'top', color = 'gray')
plt.axis('off')
plt.show()





p = N[0].float()
p = p / p.sum()
p


g = torch.Generator().manual_seed(2147483647)
# g = torch.Generator().manual_seed(37)
ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
itos[ix]


g = torch.Generator().manual_seed(2147483647)
p = torch.rand(3, generator = g)
p = p / p.sum()
p


torch.multinomial(p, num_samples=20, replacement=True, generator=g)


p.shape


g = torch.Generator().manual_seed(2147483647)
for _ in range(10):
    out = []
    ix = 0
    while True:
        p = N[ix].float()
        p = p / p.sum()
        # p = torch.ones(27) / 27.0
        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
        if ix == 0:
            break
        else:
            out.append(itos[ix])   
    print(''.join(out))





P = (N + 1).float()


P.sum(0)


P.sum(1)





P.sum(0).shape


# P.sum(0, keepdim=True) 算的是什么呢
# 以某字母结束的组合的总数，但是和某字母开始的组合的总是是一致的，所以这里其实是某字母出现的总数
# P.sum(1, keepdim=True).shape
P.sum(1, keepdim=True)


# 这里的除法是个啥
# 根据 Pytorch boradcasting semantics 原则这里会把被除张量扩展成和 P 相同的 size, 然后挨个去除
# 所以呢这里算的是每个字母组合中以开头这个字母为总数的比例
P = P / P.sum(1, keepdim=True)


P[0].sum()


g = torch.Generator().manual_seed(2147483647)
for _ in range(5):
    out = []
    ix = 0
    while True:
        p = P[ix]
        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
        if ix == 0:
            break
        else:
            out.append(itos[ix])   
    print(''.join(out))





log_likelihood = 0.0
n = 0
for w in ["andrejq"]:
    chs =['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        prob = P[ix1, ix2]
        logprob = torch.log(prob)
        log_likelihood += logprob
        n += 1
        print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')

print(f'{log_likelihood=}')
nll = -log_likelihood
print(f'{nll=}')
print(f'{nll/n}')



