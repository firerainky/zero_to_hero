import torch
import matplotlib.pyplot as plt
%matplotlib inline


words = open('names.txt', 'r').read().splitlines()


len(words)


words[1:10]


max(len(w) for w in words)


b = {}
for w in words[:10]:
    chs = ['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        bigram = (ch1, ch2)
        b[bigram] = b.get(bigram, 0) + 1


N = torch.zeros((27, 27), dtype = torch.int32)


chars = sorted(list(set(''.join(words))))
stoi = {s:i+1 for i, s in enumerate(chars)}
stoi['.'] = 0
itos = {i:s for s, i in stoi.items()}
b = {}
for w in words:
    chs = ['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        N[stoi[ch1], stoi[ch2]] += 1
        


N[0].shape


p = N[0].float()
p / p.sum()


g = torch.Generator().manual_seed(2147483647)
torch.multinomial(p, num_samples = 1, replacement = True, generator = g).item()


g = torch.Generator().manual_seed(2147483647)
p = N / N.sum(1, keepdim = True)
for _ in range(20):
    w = []
    cur_idx = 0
    while True:
        char_idx = torch.multinomial(p[cur_idx], num_samples = 1, replacement = True, generator = g).item()
        char = itos[char_idx]
        if (char == '.'):
            break
        else:
            w += char
            cur_idx = char_idx
    word = ''.join(w)
    print(word)


# 接下来来看我们要如何找到一个 loss function, 首先查看我们 training set 中的概率函数是什么样的
P = N.float()
P /= P.sum(1, keepdim = True)
# P 是我们的概率矩阵
# 接下来我们来计算一下训练集中前三个名字的概率情况是啥样的
log_likelihood = 0.0
n = 0
for word in words[:3]:
    chs = ['.'] + list(word) + ['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        prob = P[stoi[ch1], stoi[ch2]]
        logprob = torch.log(prob)
        log_likelihood += logprob
        n += 1
        print(f'{ch1}{ch2}, prob={prob:.4f}, logprob={logprob:.4f}')

print(f'{log_likelihood=}')
nll = -log_likelihood
print(f'{nll=}')
average_nll = nll / n
print(f'{average_nll=}')


# 有了判断一个生成好不好的函数之后，接下来做什么呢？
# create the training set of all the bigrams, 也就是创建训练集
xs = []
ys = []
for w in words[:1]:
    chs = ['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chs, chs[1:]):
        xs.append(stoi[ch1])
        ys.append(stoi[ch2])

xs = torch.tensor(xs)
ys = torch.tensor(ys)



xs


ys


# 但是这样的 xs 和 ys 并不能直接作为训练内容输入给 neural network
# 我们需要把他们转变成神经网络需要的 tensor, 为什么这个直给不行我暂时还没有答案
xs.shape


import torch.nn.functional as F
xenc = F.one_hot(xs, num_classes = 27).float()
xenc


xenc.shape


xenc.dtype


plt.imshow(xenc)
plt.show()


W = torch.randn((27, 27))
xenc @ W # ((5, 27) @ (27, 27)) = (5, 27), 27 neurons, @ - dot product


xenc[3] * W[:, 13] # 响应的, * - element-wise multiplication


(xenc[3] * W[:, 13]).sum()


(xenc @ W)[3, 13]


# 让我们回到构建 neural network 来吧
# randomly initialize 27 neuron's weights, each neuron receives 27 inputs
g = torch.Generator().manual_seed(2147483647)
W = torch.randn((27, 27), generator = g)


# xs 是 neural network 的 input, 下面的就是 neural network 的 forward pass
xenc = F.one_hot(xs, num_classes = 27).float() # input to the network: one-hot encoding
logits = xenc @ W # predict log-counts
counts = logits.exp() # counts, equivalent to N 
probs = counts / counts.sum(1, keepdim = True) # probabilities to next character, normalize
# btw: the last 2 lines here are togather called a "softmax"
probs.sum(1)


probs.shape


probs[0, 5], probs[1, 13], probs[2, 13], probs[3, 1], probs[4, 0]


loss = -probs[torch.arange(5), ys].log().mean()
loss


xs, ys = [], []
for w in words:
    chars = ['.'] + list(w) + ['.']
    for ch1, ch2 in zip(chars, chars[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        xs.append(ix1)
        ys.append(ix2)

xs = torch.tensor(xs)
ys = torch.tensor(ys)
num = xs.nelement()
print('number of examples: ', num)


# gradient descent
for k in range(10):
    # xs 是 neural network 的 input, 下面的就是 neural network 的 forward pass
    xenc = F.one_hot(xs, num_classes = 27).float() # input to the network: one-hot encoding
    logits = xenc @ W # predict log-counts
    counts = logits.exp() # counts, equivalent to N 
    probs = counts / counts.sum(1, keepdim = True) # probabilities to next character, normalize
    # btw: the last 2 lines here are togather called a "softmax"
    loss = -probs[torch.arange(num), ys].log().mean()
    print(loss.item())

    # backward pass
    W.grad = None
    loss.backward()

    # update
    W.data = -0.1 * W.grad
