import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
%matplotlib inline


# read in all the words
words = open('names.txt', 'r').read().splitlines()
len(words)


words[:8]


# build the vocabulary of characters to/from integers
chars = sorted(list(set(''.join(words))))
stoi = {s:i+1 for i, s in enumerate(chars)}
stoi['.'] = 0
itos = {i:s for s, i in stoi.items()}
print(itos)


# build the dataset
block_size = 3
X, Y = [], []

for w in words:
    context = [0] * block_size
    w = w + '.'
    for ch in list(w):
        ix = stoi[ch]
        X.append(context)
        Y.append(ix)
        context = context[1:] + [ix]
X = torch.tensor(X)
Y = torch.tensor(Y)


X.shape, Y.shape


C = torch.randn(27, 2)
C[5]


F.one_hot(torch.tensor(5), num_classes=27).float() @ C


C[X].shape


X[13, 2]


C[1]


emb = C[X]
emb.shape


W1 = torch.randn(6, 100)
b1 = torch.randn(100)


emb @ W1 + b1


torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1).shape


torch.cat(torch.unbind(emb, 1), 1).shape


# emb.view(32, 6) == torch.cat(torch.unbind(emb, 1), 1)
emb.view(32, 6).shape


h = torch.tanh(emb.view(-1, 6) @ W1 + b1)


h


h.shape


W2 = torch.randn(100, 27)
b2 = torch.randn(27)


logits = h @ W2 + b2
logits.shape


counts = logits.exp()


prob = counts / counts.sum(1, keepdims=True)
prob.shape


prob[0].sum()


loss = -prob[torch.arange(32), Y].log().mean()
loss


# ------------- now make everything respectable :) -----------------


X.shape, X.dtype, Y.shape, Y.dtype # dataset


g = torch.Generator().manual_seed(2147483647)
C = torch.randn((27, 2), generator = g)
W1 = torch.randn((6, 100), generator = g)
b1 = torch.randn(100, generator = g)
W2 = torch.randn((100, 27), generator = g)
b2 = torch.randn(27, generator = g)
parameters = [C, W1, b1, W2, b2]


sum(p.nelement() for p in parameters) # the number of parameters in total


for p in parameters:
    p.requires_grad = True


for _ in range(1000):
    # mini batch construct
    ix = torch.randint(0, X.shape[0], (32,))
    
    # forward pass
    emb = C[X[ix]] # [32, 3, 2]
    h = torch.tanh(emb.view(-1, 6) @ W1 + b1) # [32, 100]
    logits = h @ W2 + b2 # [32, 27]
    # counts = logits.exp()
    # prob = counts / counts.sum(1, keepdims = True)
    # loss = -prob[torch.arange(32), Y].log().mean()
    loss = F.cross_entropy(logits, Y[ix])
    # backward
    for p in parameters:
        p.grad = None
    loss.backward()
    # print(loss.item())
    # update
    for p in parameters:
        p.data += -0.1 * p.grad
        
print(loss.item())


X.shape, ix.shape, X[ix].shape


logits.max(1)


Y


logits.shape



